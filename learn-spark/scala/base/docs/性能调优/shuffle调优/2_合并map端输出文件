### 默认不合并的情况
第一个 stage, 每个 task, 都会给第二个 stage 的每个 task 创建一份 map 端的输出文件
第二个 stage, 每个 task, 都会到各个节点上面去,拉取第一个 stage 每个 task 输出的属于自己的那一份文件
#### 实际生产环境条件:
100 个节点(每个节点一个 executor): 100 个 executor
每个 executor: 2 个 CPUcore
总共 1000 个task: 平均每个 executor 10个 task

每个节点, 10个 task, 每个节点会输出的map端文件数量:
10 * 1000 = 10000 个文件

总共要输出的文件数量:
100 * 10000 = 100万

#### 说明
shuffle 中的写操作,基本上就是 shuffle 中性能消耗最为严重的部分,通过上面的分析,一个普通的生产环境的 spark job 的一个 shuffle 环节,会写入磁盘
100 万个文件, 基本上,spark 作业的性能都消耗在 shuffle 中了.

### 合并map端文件的情况
```
new SparkConf().set("spark.shuffle.consolidateFiles", "true")
```
开启了 map 端输出文件的合并机制之后:
第一个 stage, 同时就运行 CPUcore 个 task, 比如 CPUcore 是 2个,并行运行 2 个 task,每个task 都创建下一个 stage 的 task 数量个文件
第二个 stage, task 再拉取数据的时候,就不会去拉取上一个 stage 每个task 为自己创建的那份输出文件,而是拉取少量的输出文件,每个输出文件中,可能
包含了多个 task 给自己的 map 端输出

只有并行执行的 task 会去创建新的输出文件;下一批并行执行的 task,就会去复用之前的已有的输出文件;但是有一个例外,比如 2 个 task 并行在执行,但是
此时又启动要执行 2 个task;那么这个时候,就无法复用刚才的 2 个 task 创建的输出文件了;而是只能去创建新的输出文件
要实现输出文件的合并的效果,就必须是一批task先执行,然后下一批task在执行,才能复用之前的输出文件;负责多批 task 同时起来执行,还是做不到复用

#### 生产环境的变化
100个节点(每个节点一个 executor): 100个 executor
每个 executor: 2个 cpucore
总共 1000 个 task: 每个 executor 平均 10个 task

每个节点 2个 cpucore,输出文件的数量是:
2*1000 = 2000个
总共 100个 节点,创建的输出文件的数量是:
100 * 2000 = 20 万

相较之前未开启合并机制的时候,数量是开启合并机制的 5 倍

#### 开启map端合并机制对 spark 性能的影响
1. map task 写入磁盘文件的 IO 减少
2. 第二个 stage,原本要拉取第一个 stage 的task 数量份的文件数量(1000份),走网络传输;合并之后,100个节点,每个节点 2个 CPUcore,第二个 stage
的每个 task,主要拉取 100 * 2 = 200 个文件即可,网路传输的性能也大大减少


