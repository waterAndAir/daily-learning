## 笔记&讨论
### 入门
#### 为什么要给每一个 pod 初始一个 pause 容器
1. 以业务无关的 pause 容器的状态代表 pod 的状态
2. 方便 pod 中的容器共享网络和存储 

#### 理解最小单元 pod
1. pod 中任意一个容器停止,kubernetes 都会自动的重启该 pod
2. pod 所在节点宕机后,该节点的所有 pod 将重新调度到其他节点

#### Label Selector 的一些使用场景
1. kube-controller 通过 RC 定义的 LabelSelector 筛选要监控的 Pod 副本数,从而实现 Pod 副本数始终符合预期设定的自动控制流程
2. kube-proxy 通过 Service 的 Label Selector 选择对应的 Pod, 自动建立 Service 到 Pod 的请求转发路由表,实现负载均衡
3. 对 Node 定义特定Label, 并在 Pod 定义中使用 NodeSelector,实现 Pod 的定向调度

#### ReplicationController(RC)的一些特性与作用,Replica Set 与之相同
- 大部分情况下,通过定义 RC 实现 Pod 的管理
- RC 里包含完整的 Pod 定义模板
- RC 通过 Label Selector 实现对Pod 副本的自动控制
- 通过改变 RC 里的 Pod 副本数,可以实现 Pod 的扩容和缩容
- 通过改变 RC 里的 Pod 模板中的镜像版本,可以实现 Pod 的滚动升级
#### Replication Controller 和 Replica Set 的区别
Replica Set 支持基于集合的 Label selector (Set-based selector)  
Replication Controller 只支持基于等式的 Label Selector(equality-based selector)  

Replica Set 很少单独使用,它主要被 Deployment 这个更高层的资源对象使用.  
Replica Set 和 Department 逐步替代了 RC的作用

#### Deployment 的使用场景
- 创建一个 Deployment 来生层对应的 Replica Set 并完成副本的创建过程
- 检查 Deployment 状态看部署是否完成(Pod副本的数量是否达到预期的值)
- 更新 Deployment 以创建新的 Pod (比如镜像升级)
- 如果当前 Deployment 不稳定,则回滚到一个早先的 Deployment 版本
- 挂起或者恢复一个 Deployment

#### Horizontal Pod Autoscaler (HPA) 是什么?
HPA 也是一个资源对象, 它表示 Pod 横向自动扩容: 根据目标 Pod 的负载情况,自动的调整目标 Pod 的副本数.  
HPA 可以根据两行方式作为指标:  
1. CPUUtilizationPercentage, 表示目标 Pod 所有副本的 Cpu 利用率的平均值. 
2. 应用程序自定义的度量指标,比如 TPS 或 QPS      

注意:  
- Pod CPU 使用率来源于 heapster, 要预先安装
- 在 RC 或 Deployment 中的 Pod 必须定义 `resources.requests.cpu`

#### Kubernetes 里的"三种 IP"
##### NodeIP: Node 节点的 IP 地址
Kubernetes 集群中每个节点的物理网卡IP地址, Kubernetes 集群外的节点通过 NodeIP 访问 Kubernetes 集群中某个节点的服务.
##### PodIP: Pod 的 IP 地址
它是 Docker Engine 根据 docker0 网桥的IP地址段进行分配的,通常是一个虚拟的二层网络.
Kubernetes 里一个 Pod 里的容器访问另一个 Pod 里的容器,就是通过 PodIP 所在的虚拟二层网络进行通信,
而真实的 TCP/IP 流量则是通过 NodeIP 所在的物理网卡流出的.

##### ClusterIP: Service 的 IP 地址
虚拟的IP
- ClusterIP 仅仅作用于 Kubernetes Service 这个对象,并由 Kubernetes 管理和分配 IP 地址(来源于 ClusterIP 地址池)
- ClusterIP 无法并Ping, 因为没有一个"实体网络对象"来响应
- ClusterIP 只能结合 Service Port 组成一个具体的通信端口,单独的 ClusterIP 不具备 TCP/IP 通信的基础.
- 在 Kubernetes 集群中,NodeIP网,PodIP网与ClusterIP网之间的通信,采用的是 Kubernetes 自己设计的一种变成方式的特殊的路由规则

#### Service 的 NodePort 方式实现集群外部访问服务
实现方式: 在 Service 的 `spec` 中制定 `type` 为 `NodePort`, 并在 `spec.ports` 中定义 `nodePort`  

注意: 外部系统可以用kubernetes集群中任意一个 NodeIP + 具体NodePort 访问内部服务.

#### 定义在Pod上的Volume类型
##### emptyDir
emptyDir Volume 是在 Pod 分配到 Node 时创建的,Kubernetes 自动为其在宿主机上分配一个目录.
当 Pod 从 Node 上移除时, emptyDir 中的数据也会被永久删除.  

应用场景:
- 临时空间,某些应用程序运行时所需要的临时目录,且无需永久保留
- 长时间任务的中间过程 CheckPoint 的临时保存目录
- 一个容器需要从另一个容器中获取数据的目录(多容器共享目录)
##### hostPath
为Pod挂在宿主机上的文件或目录.  

应用场景:
- 容器应用程序生成的需要永久保存的日志文件
- 需要访问宿主机上Docker引擎内部数据结构的容器应用时,可以定义 hostPath 为 /var/lib/docker 目录

注意:  
- 在不同的 Node 上具有相同配置的 Pod 可能会因为宿主机上的目录和文件不同而导致对 Volume 上的目录和文件的访问结果不一致
- 如果使用了资源配额管理,则 Kubernetes 无法将 hostPath 在宿主机上使用的资源纳入管理

##### NFS 网络文件系统

##### 其它
- gcePersistentDick 谷歌公有云的永久磁盘
- awsElasticBlockStore 亚马逊公有云提供的 EBS Volume 存储 
- ...

#### Persistent Volume(PV) 与 Persistent Volume Claim(PVC)
PV 是一种资源对象; PVC 是创建 PV 的模板. 

- PV 只能是网络存储,不属于任何 Node, 但可以在每个 Node 上访问
- PV 不是定义在 Pod 上的,而是独立于 Pod 之外的定义
- PV 类型: NFS,RBD,GCE Persistent Disks,iSCSCI,AWS ElasticBlockStore,GlusterFS 

#### ConfigMap 供容器使用的典型用法和限制条件
##### 典型用法
- 生成为容器内的环境变量
- 设置容器启动命令的启动参数(需设置为环境变量)
- 以 Volume 的形式挂载为容器内部的文件或目录
##### 限制条件
- ConfigMap 必须在 Pod 之前创建
- ConfigMap 也可以定义为属于某个 Namespace, 只有处于相同 Namespaces 中的 Pod 可以引用它
- 只能用于被 Api Server 管理的 Pod 使用,静态 Pod 无法引用 ConfigMap
- Pod 对 ConfigMap 进行挂载(VolumeMount)时,只能被挂载为目录,而不是文件,且会覆盖该目录其他文件.如果要保留其他文件,可以先将 ConfigMap 挂载到临时目录,在通过启动脚本将配置文件复制到 实际配置目录下

### Pod
#### Pod 的几种状态
- Pending: API Server 已经创建该 Pod, 但 Pod 内还有一个或多个容器的镜像没有创建,包括正在下载镜像的过程
- Running: Pod 内所有容器均已创建,且至少有一个容器处于运行,正在启动,或者正在重启状态
- Succeed: Pod 内所有容器均成功执行退出,且不会再重启
- Failed: Pod 内所有容器均已退出,但至少有一个容器退出为失败状态
- Unknown: 无法获取该Pod的状态,可能由于网络通信问题导致

#### Pod 的重启策略(Restart Policy)
由Pod所在Node上的kubelet进行判断,当容器异常退出或者健康检查失败后,根据Pod设置的重启策略进行操作.  
重启策略包括:  
- Always: 容器失效时,自动重启
- OnFailure: 容器终止运行且退出码不为0时,自动重启
- Never: 不会重启  

用于管理 Pod 的控制器对Pod重启策略的要求:  
- RC 和 DaemonSet: 必须为 Always,保证容器持续运行
- Job: OnFailure 或 Never, 确保容器执行完后不再重启
- kubelet 命令行: 失效时自动重启,不管 RestartPolicy 设置为什么值,也不会对 Pod 进行健康检查

#### Pod 健康检查
- LivenessProbe 探针: 判断容器是否存活(running 状态), 如果容器不包含 LivenessProbe, 则始终认为是 "success"
- - ExecAction: 在容器内部执行一个命令,返回为0, 则表明容器健康
- - TcpSocketAction: 判断是否可以通过容器的 IP 和端口号建立TCP连接
- - HTTPGetAction: 判断容器的http服务的一个Get请求是否返回大于200小于400的状态码
- ReadinessProbe: 判断容器是否启动完成(ready状态), 可以接收请求.

#### Pod 的调度
##### RC,Deployment: 全自动调度
维护多份副本
##### DaemonSet: 特定场景调度
用于管理在集群中每个 Node 上仅运行一份 Pod 的副本实例, 比如:  
- 在每个 Node 上运行一个 GlusterFS 存储或者 Ceph 存储的 daemon 进程
- 在每个 Node 上运行一个日志采集程序, 例如 fluentd 或者 logstach
- 在每个 Node 上运行一个健康程序,采集该Node的运行性能数据,例如 Prometheus Node Exporter, collectd, New Relic agent 或者 Ganglia gmond 等

##### Job: 批处理调度

###### Non-parallel Jobs
通常一个 Job 只启动一个 Pod,只有在Pod异常后,才会重启该Pod,Pod正常结束,Job也将结束

###### Parallel Jobs With a fixed completion count
- 并行 Job 会启动多个 Pod
- 当正常结束的 Pod 数量达到 `spec.completions` 时,Job结束.
- `spec.parallelism` 参数控制并行度,表示同时起送几个 Job 来处理 Work Item

每个 Pod 对应一个工作项,处理完一个,Pod就结束了,这种方式会大量的结束和开启 Pod

###### Parallel Jobs with a work queue
- 需要一个存放 Work item 的独立的 Queue
- 不能设置 `spec.completions` 参数
- 每个 Pod 能独立判断和决定是否还有任务项需要处理
- 如果某个 Pod 正常结束,则 Job 不会再启动新的 Pod,其他Pod 应该处于即将结束或者退出状态
- 所有Pod都结束了,且至少有一个 Pod 成功结束,则整个Job算成功结束  

每个 Pod 不断从对垒中拉取工作项并处理,知道队列为空,Pod退出执行,因此,这种情况下,只要有一个 Pod 成功结束,就意味着整个 Job 进入终止状态

#### 滚动升级(Rolling Update)的流程 todo
新建一个新的 RC: `new-rc.yaml`, 运行 `kubectl rolling-update <old-rc> -f new-rc.yaml`  

注意:  
- 新的 RC 的名称不能与旧的 RC 名称相同  
- 新的 RC 在 Selector 中应至少有一个 label 与旧的 RC 的 Label 不同,以标识新的RC

### Service
#### 什么是 Headless Service   
在某些场景中,开发人员希望自己控制负载均衡的策略,不使用 Service 提供的默认负载均衡,这时就可以通过 Headless Service 实现.   

将 Service 的 Cluster 设置为 None,通过 Label Selector 将后端的 Pod 列表返回给调用的客户端,由客户端程序自己实现负载均衡,确定范文哪一个后端 Pod

#### 什么是无 Label Selector 的服务  
某些环境中,kubernetes 中的服务需要连接一个外部数据库,或者连接另一个集群或namespace 的服务,这时可以通过创建一个无 Label Selector 的 Service 实现.  

定义一个无LabelSelector的Service,就无法选择后端 Pod, 因此需要先手动创建一个 Endpoint,

#### kubernetes 外部访问 Pod 或 Service 的方式

##### 将容器应用的端口号映射到物理机(不推荐,因为Pod的IP和所在的Node是随时会变的)
1. 设置容器级别的 hostPort, 将容器应用的端口号映射到物理机上,然后通过物理机 IP + hostPort 就可以访问Pod内的服务  
2. 设置 Pod 级别的 hostNetwork= true, 该 Pod 中所在的容器端口号都会被映射到物理机上.

##### 将 Service 的端口号映射到物理机 
1. 通过设置 `spec.type=NodePort`,且在 `spec.ports.port.nodePort` 指定映射到物理机的端口, 通过 NodeIP + NodePort 可以访问服务
2. 设置 LoadBalancer,映射到负载均衡的IP地址,一般仅用于云平台

#### Ingress: HTTP 7 层路由机制
对于基于 http 的服务来说,仅仅通过 Service 的 IP:Port 形式不能满足不同url对应不同服务的需求.  
比如:
- 对 http://website/api 的访问要路由到 名为 api 的 Service(http://api:80)
- 对 http://website/web 的访问要路由到 名为 web 的 Service(http://web:80)
- 对 http://website/docs 的访问要路由到 名为 docs 的 Service(http://docs:80)  

这种场景可以用 Ingress 解决, 步骤:  
##### 创建 Ingress Controller
Ingress Controller 实现基于不同 HTTP URL 向后转发的负载分发规则,有的公有云也提供了这种类型的 LoadBalancer,可以将其设置为 Ingress Controller.  

可以用 nginx 实现一个 Ingress Controller.  

##### 定义 Ingress

### Kubernetes API Server
- 提供了各类资源对象的curd及watch 等 http rest 接口,是数据交互和通信的中心枢纽
- 是集群管理的API入口
- 是资源配额控制的入口
- 提供了完备的集群安全机制
- kubectl 就是通过调用 kube-apiserver 提供的 rest 接口来进行操作的,因此,也可以使用 curl 替代 kubectl

### Controller Manager 管理控制中心
负责集群内的Node,Pod副本,Endpoint,Namespace,ServiceAccount,ResourceQuota 等的管理,  
它会及时发现故障并自动修复,确保集群始终处于预期的工作状态,是核心管理者.  


#### Replication Controller: 副本控制器, 注意区别同样称为 Replication Controller(简称 RC) 的一种资源对象,
职责:  
- 自动调度: 确保当前集群中有且仅有 N 个Pod实例
- 弹性伸缩: 通过调整 replicas 属性实现系统扩容或缩容
- 滚动更新: 通过修改 RC 中 Pod 模板(主要是镜像版本)来实现系统的更新,通过创建一个新的 RC,实现滚动更新,新RC的副本数逐步加1,旧 RC 的副本数逐步减1

#### Node Controller  

kubelet 进程在启动时通过 API server 注册自身节点信息,并定时汇报状态信息.    

NodeController 通过 API Server 实时获取Node信息,实现管理和监控集群中的各个 Node 节点的相关控制功能.  

#### ResourceQuota Controller 资源配额控制器
确保指定的资源对象在任何时候都不会超量占用系统物理资源,确保集群的稳定性.  

目前,支持三个层次的资源配额管理:  
- 容器级别,对 cpu 和 memory 进行限制
- Pod 级别,对 Pod 内所有容器的可用资源进行限制(通过 LimitRanger 设置)
- Namespace 级别,包括 Pod 数量,Replication Controller 数量,Service 数量,ResourceQuota 数量,Secret 数量,可是持有的 Persistent Volume 数量(通过 ResourceQuota 设置)

#### Namespace Controller  

管理 Namespace 的 创建和删除, 删除的同时,会删除该 Namespace 下的各种资源对象

#### Endpoint Controller
Endpoints 表示一个 Service 对应的所有 Pod 副本的访问地址, EndpointsController 就是负责生成和维护所有 Endpoints 对象的控制器. 它负责
监听 Service 和 对应副本的变化:  
- 如果 Service 被删除,则删除和该 Service 同名的 Endpoints 对象  
- 如果 Service 被创建,则根据 Service 信息获取 Pod 列表,然后创建或者更新 Service 对应的 Endpoints 对象  
- 如果检测到 Pod 时间,则更新它所对应的 Service 的 Endpoints 对象  

Endpoints 对应被每个 Node 上的 kube-proxy 进程使用,kube-proxy 进程获取每个 Service 的 Endpoints, 实现了 Service 的负载均衡.

#### Service Controller  

它其实是 Kubernetes 集群与外部的一个接口控制器,Service Controller 监听 Service 的变化,如果是一个 LoadBalancer 类型的Service,就确保
外部云平台上该 Service 对应的 LoadBalancer 实例被相应的创建,删除以及更新路由转发表.  

### Scheduler  





## 注意
### 前台运行启动命令
kubernetes 要求我们自己创建的docker镜像以一个前台命令作为启动命令,如果在后台运行,比如 `nohup ./start,sh &`, 该命令执行完后,会销毁该Pod.  

对于无法改造为前台执行的应用,可以使用 supervisor.

### Endpoint
PodIP + containerPort = Endpoint, 一个 Pod 可以有多个 Endpoint

### 删除 RC 控制的 pod
删除一个 RC 不会影响它所创建的 Pod, 如果想删除一个 RC 所控制的 Pod, 需要将该 RC 的 replicas 设置为 0, 这样所有的 Pod 副本都会被自动删除.  
