### 介绍
map 端的 task 是不断的输出数据的,数据量可能是很大的
但是,其实 reduce 端的 task,并不是等到 map 端 task 将属于自己的那份数据全部写入磁盘后才去拉取的.map 端写一点数据,reduce 端 task 就会拉取
一小部分数据,立即进行后面的聚合,算子函数的应用

每次 reduce 能够拉取多少数据,就由 buffer 来决定.因为 拉取过来的数据,都是先放到 buffer 中的,然后才用后面的executor 分配的堆内存占比(0.2),
去进行后续的聚合,函数的执行
### reduce 端缓冲可能会出现的问题
默认是 48MB ,也许大多数的时候,reduce 端 task 一边拉取一边计算,不一定会都拉慢 48 MB 的数据,可能大多数的时候,拉取 10MB 就计算掉了

大多数时候,不会出现什么问题,但是有时候,map 端的数据量特别大,然后写出的速度特别快,reduce 端所有task,拉取的时候,全部都达到自己的缓冲的最大极限
值,全部填满

这个时候, 再加上 reduce 端执行的聚合函数的代码,可能会创建大量的对象,也许,一下子,内存就耗尽了,发生 OOM.

### 解决
这个时候,就应该减少 reduce 端 task 缓冲的大小,宁愿多拉取几次,但是每次同时能够拉取到 reduce 端的每个 task 的数量,比较少,就不容易发生 OOM

这是典型的以性能换执行的原理,reduce 端缓冲小了,不容易 OOM 了,但是性能一定是有所下降的,拉取的次数多了,就会有更多的网络开销

这种时候,只能采用牺牲性能的方式了.

### 调优
如果 map 端输出的数据量不是特别大,这个呢个 application 的资源也很充足,可以尝试增加这个 reduce 端缓冲大小,比如从 48 M, 变成 96M.这样的话,
每次 reduce task 能够拉取的数据量就很大,需要拉取的次数也就变少了,对网络传输的开销减少,以及 reduce 端聚合操作执行的次数减少

### 设置方式
`spark.reducer.maxSizeInFlight，48
 spark.reducer.maxSizeInFlight，24``
```
