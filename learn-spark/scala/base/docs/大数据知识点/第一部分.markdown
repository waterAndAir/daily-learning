### 离线日志采流程

- 网站,App发送请求到后台服务器.(高并发情况下通常会用nginx做负载均衡),对服务器进行适当的配置之后,关于请求相应的数据就会存到日志文件中,
后台系统(java,php,python)也会根据业务逻辑按照制定的规范,记录日志到文件中.
- 通常每个日志文件是按照"天"来划分的,所以,一台web服务器上,就至少有一个日志文件
- 设计一个定时任务,完成日志文件的转移,转移到 flume agent 正在监控的目录中,并定时执行该脚本
- flume agent 监控到 linux系统上面的某一个文件夹,发现有新的日志文件进来,flume就会走后续的channel和sink,通常sink都会配置为hdfs
如果是多台服务器.则每台服务器上的flume把sink设置为某个用于合并日志文件的flume的agent上
- 进行数据清洗,使用 hadoop 的 mr 作业 或 hive 作业, 进行数据清洗,写入另外一个hdfs文件
- 把清洗后的数据,导入到hive表中,这里可以使用动态分区,hive 使用分区表,每个分区存放一天的数据
- Hive 作为一个大数据的数据仓库,后续要进行一些数据仓库建模的 ETL , 将原始日志所在的一个表,转换成 几十张,甚至几百张.
- 数据分析人员针对数据仓库中的表,执行临时的,或者每天定时执行 hive sql etl 作业,进行大数据统计分析
- spark, storm, hadoop 等大数据平台都有可能会用到 hive 中的数据仓库内部的表