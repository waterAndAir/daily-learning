### 现象
如果是基于 YARN 来提交 spark, 比如 yarn-cluster 或者 yarn-client.可以指定提交到 hadoop 队列上,每个队列都是可以有自己的资源的
比如:
给 spark 用的 yarn 资源队列的情况: 500G 内存, 200个 cpucore
某个 spark application, 在 spark-submit 里面配置了 executor 80个;每个 executor 4G 内存;每个 executor 2个 CPUcore. spark 作业每次
运行大概要消耗掉 320G 内存,以及 160个 CPUcore.

数字上看队列资源是足够的: 500G 内存, 200个 cpucore

问题是:
spark 作业实际运行起来后,消耗掉的资源,可能比在 spark-submit 中配置的要大一些,可能 400G 内存, 190个 cpucore
这个时候,队列资源还是有一些剩余的,但是,如果同时又提交了一个 spark 资源上去,可能就会出现问题

可能会出现两种情况:
1. YARN. 发现资源不足时,spark作业并没有 hang 在那里,等待资源的分配,而是直接打印一行 fail 的 log,直接就失败了
2. YARN, 发现资源不足时,spark作业 hang 在那里,一直等待之前的 spark 作业执行完,等待有资源分配给自己来执行

### 解决:
1. 限制同时只允许一个 spark 作业在运行
2. 应该采用一些简单的调度区分的方式,比如说,有的 spark 作业可能要长时间运行,比如 30 分钟.有的 spark 作业可能是短时间运行的,可能就运行 2 分钟.
此时,都提交到一个队列上去,肯定不合适,很可能出现 30 分钟的作业卡住后面一大堆 2 分钟的作业.
可以自己做来两个 调度队列,每个队列根据要执行的作业的情况来设置
3. 队列里面,无论何时,只会有一个作业在里面运行,那么此时,就应该用将每个队列能承载的最大资源,分配给一个 spark 作业
比如: 80个 executor；6G 内存; 3个 cpucore.
尽量让每一次运行,都达到最满的资源使用率,最快的速度,最好的性能,